{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Cross-Entropy Loss and Multi-Label Loss\n",
    "\n",
    "Cross-Entropy Loss and Multi-Label Loss are fundamental concepts in machine learning and deep learning for classification tasks. These loss functions are crucial for training models to make accurate predictions. Here, we will explore both, explaining their purposes, differences, and how they are mathematically formulated.\n",
    "\n",
    "## Cross-Entropy Loss\n",
    "\n",
    "Cross-Entropy Loss is primarily used in multi-class classification problems. It measures how well the predicted probability distribution over all classes matches the actual distribution. A higher divergence between the predicted and actual distributions results in a higher loss, making it an effective measure for classification models.\n",
    "\n",
    "The Cross-Entropy Loss for a single observation and `C` classes is given by:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{c=1}^{C} y_{o,c} \\log(p_{o,c})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- `L` is the loss for one observation,\n",
    "- `C` is the number of classes,\n",
    "- `y_{o,c}` is a binary indicator (0 or 1) if class `c` is the correct classification for observation `o`,\n",
    "- `p_{o,c}` is the predicted probability of observation `o` being of class `c`.\n",
    "\n",
    "## Multi-Label Loss\n",
    "\n",
    "For scenarios where an observation can be associated with multiple classes simultaneously, Multi-Label Loss is used. It differs from Cross-Entropy Loss, which assumes each observation is associated with a single class.\n",
    "\n",
    "A common approach to Multi-Label Loss is using Binary Cross-Entropy Loss, calculated for each class as an independent binary classification. The formula for `N` samples and `C` classes is:\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{c=1}^{C} [y_{n,c} \\log(\\sigma(x_{n,c})) + (1 - y_{n,c}) \\log(1 - \\sigma(x_{n,c}))]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- `N` is the number of samples,\n",
    "- `C` is the number of classes,\n",
    "- `y_{n,c}` is a binary indicator if class `c` is the correct classification for sample `n`,\n",
    "- `x_{n,c}` is the raw output of the model for class `c` for sample `n`,\n",
    "- `\\sigma` denotes the sigmoid function, which converts the raw output into a probability.\n",
    "\n",
    "Understanding and applying these loss functions correctly is vital for improving the performance of classification models, enabling them to make accurate predictions across a variety of tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simulated logits from a neural network (for 2 samples and 3 labels)\n",
    "logits = torch.tensor([[0.1, -0.2, 0.0], [0.4, 0.1, -0.3]])\n",
    "\n",
    "# Corresponding ground truth labels (1 indicates the label is present, 0 indicates the label is absent)\n",
    "targets = torch.tensor([[1, 0, 1], [0, 1, 0]], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelSoftMarginLoss: 0.6745749711990356\n"
     ]
    }
   ],
   "source": [
    "# Define the MultiLabelSoftMarginLoss\n",
    "loss_fn = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fn(logits, targets)\n",
    "print(f\"MultiLabelSoftMarginLoss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.5002, 0.9089])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([50, 0.0006, 2.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7150, 0.5866, 0.7006])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([0.92, 0.35, 0.85]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5250, 0.4502, 0.5000],\n",
      "        [0.5987, 0.5250, 0.4256]])\n",
      "tensor([[0.6444, 0.5981, 0.6931],\n",
      "        [0.9130, 0.6444, 0.5544]])\n",
      "Binary Cross-Entropy Loss: 0.6745750308036804\n"
     ]
    }
   ],
   "source": [
    "# Manually apply sigmoid to convert logits to probabilities\n",
    "probabilities = torch.sigmoid(logits)\n",
    "print(probabilities)\n",
    "# Compute binary cross-entropy loss\n",
    "bce_loss_fn = nn.BCELoss(reduction='none')\n",
    "bce_loss = bce_loss_fn(probabilities, targets)\n",
    "print(bce_loss)\n",
    "print(f\"Binary Cross-Entropy Loss: {bce_loss.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6746)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
