{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Softmax Precision Demo ---\n",
      "Softmax Output (float32): tensor([0.3775, 0.6225])\n",
      "Softmax Output (bfloat16): tensor([0.5000, 0.5000], dtype=torch.bfloat16)\n",
      "Difference: tensor([-0.1225,  0.1225])\n",
      "Percentage Error (%): tensor([-32.4361,  19.6735])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(\"\\n--- Softmax Precision Demo ---\")\n",
    "logits_fp32 = torch.tensor([128.0] + [128.5], dtype=torch.float32)\n",
    "logits_bf16 = logits_fp32.to(torch.bfloat16)\n",
    "\n",
    "softmax_fp32 = torch.nn.functional.softmax(logits_fp32, dim=0)\n",
    "softmax_bf16 = torch.nn.functional.softmax(logits_bf16, dim=0)\n",
    "\n",
    "print(\"Softmax Output (float32):\", softmax_fp32)\n",
    "print(\"Softmax Output (bfloat16):\", softmax_bf16) # since 1.00000001 x 2^7 , since bf16 has only 7 bits of mantissa it cant get the 1 in the 8th bit so it gets rounded to 128 \n",
    "print(\"Difference:\", (softmax_fp32 - softmax_bf16))\n",
    "difference = softmax_fp32 - softmax_bf16\n",
    "percentage_error = difference / softmax_fp32 * 100\n",
    "\n",
    "print(\"Percentage Error (%):\", percentage_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Matrix Multiplication Precision Demo ---\n",
      "Matrix Multiplication Result (float32):\n",
      " tensor([[ 4761508.5000,  3016870.7500,  -413075.7188, -4229289.5000,\n",
      "         -1386271.0000],\n",
      "        [ 1121975.1250, -2302104.7500, -1298255.6250,   567901.2500,\n",
      "           277495.8750],\n",
      "        [ 2676308.7500,  -825978.3750, -1779275.0000, -2562160.0000,\n",
      "          1479074.2500],\n",
      "        [-2551562.0000, -2716938.7500,  -620452.7500,  4154820.0000,\n",
      "          -229077.9688],\n",
      "        [-1324644.5000, -1441797.3750, -1320998.3750,   544282.6250,\n",
      "          2094519.1250]])\n",
      "Matrix Multiplication Result (bfloat16):\n",
      " tensor([[ 4751360.,  3014656.,  -409600., -4227072., -1384448.],\n",
      "        [ 1122304., -2293760., -1294336.,   565248.,   276480.],\n",
      "        [ 2670592.,  -823296., -1777664., -2572288.,  1482752.],\n",
      "        [-2539520., -2703360.,  -618496.,  4145152.,  -231424.],\n",
      "        [-1327104., -1433600., -1318912.,   544768.,  2097152.]])\n",
      "Difference:\n",
      " tensor([[ 10148.5000,   2214.7500,  -3475.7188,  -2217.5000,  -1823.0000],\n",
      "        [  -328.8750,  -8344.7500,  -3919.6250,   2653.2500,   1015.8750],\n",
      "        [  5716.7500,  -2682.3750,  -1611.0000,  10128.0000,  -3677.7500],\n",
      "        [-12042.0000, -13578.7500,  -1956.7500,   9668.0000,   2346.0312],\n",
      "        [  2459.5000,  -8197.3750,  -2086.3750,   -485.3750,  -2632.8750]])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Matrix Multiplication Precision Demo ---\")\n",
    "A = torch.randn((5, 5), dtype=torch.float32) * 1000  # Large values\n",
    "B = torch.randn((5, 5), dtype=torch.float32) * 1000\n",
    "\n",
    "A_bf16 = A.to(torch.bfloat16)\n",
    "B_bf16 = B.to(torch.bfloat16)\n",
    "\n",
    "C_fp32 = A @ B\n",
    "C_bf16 = A_bf16 @ B_bf16\n",
    "\n",
    "print(\"Matrix Multiplication Result (float32):\\n\", C_fp32)\n",
    "print(\"Matrix Multiplication Result (bfloat16):\\n\", C_bf16.to(torch.float32))\n",
    "print(\"Difference:\\n\", (C_fp32 - C_bf16.to(torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summation Precision Demo ---\n",
      "Summation Result (float32): 100.01000213623047\n",
      "Summation Result (bfloat16): 100.010009765625\n",
      "Difference: -7.62939453125e-06\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Summation Precision Demo ---\")\n",
    "small_numbers = torch.tensor([1e-8] * 1000000, dtype=torch.float32)\n",
    "large_number = torch.tensor([1e2], dtype=torch.float32)\n",
    "\n",
    "sum_fp32 = large_number + small_numbers.sum()\n",
    "sum_bf16 = large_number + small_numbers.to(torch.bfloat16).sum()\n",
    "\n",
    "print(\"Summation Result (float32):\", sum_fp32.item())\n",
    "print(\"Summation Result (bfloat16):\", sum_bf16.item())\n",
    "print(\"Difference:\", sum_fp32.item() - sum_bf16.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Roundoff Error Calculation ---\n",
      "Range (2, 4): BFloat16 Roundoff Error = 0.015625, Float32 Roundoff Error = 2.384185791015625e-07\n",
      "Range (32, 64): BFloat16 Roundoff Error = 0.25, Float32 Roundoff Error = 3.814697265625e-06\n",
      "Range (1024, 2048): BFloat16 Roundoff Error = 8.0, Float32 Roundoff Error = 0.0001220703125\n",
      "Range (1048576, 2097152): BFloat16 Roundoff Error = 8192.0, Float32 Roundoff Error = 0.125\n",
      "Range (1073741824, 2147483648): BFloat16 Roundoff Error = 8388608.0, Float32 Roundoff Error = 128.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Roundoff Error Calculation ---\")\n",
    "number_ranges = [(2, 4), (32, 64), (1024, 2048), (2**20, 2**21), (2**30, 2**31)]\n",
    "mantissa_bits_bf16 = 7\n",
    "mantissa_bits_fp32 = 23\n",
    "\n",
    "for r in number_ranges:\n",
    "    step_bf16 = (r[1] - r[0]) / (2 ** mantissa_bits_bf16)\n",
    "    step_fp32 = (r[1] - r[0]) / (2 ** mantissa_bits_fp32)\n",
    "    print(f\"Range {r}: BFloat16 Roundoff Error = {step_bf16}, Float32 Roundoff Error = {step_fp32}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FP32: 2.00000, Rounded BF16: 2.00000\n",
      "Original FP32: 2.01000, Rounded BF16: 2.01562\n",
      "Original FP32: 2.02000, Rounded BF16: 2.01562\n",
      "Original FP32: 2.03000, Rounded BF16: 2.03125\n",
      "Original FP32: 2.04000, Rounded BF16: 2.04688\n",
      "Original FP32: 2.05000, Rounded BF16: 2.04688\n",
      "Original FP32: 2.06000, Rounded BF16: 2.06250\n",
      "Original FP32: 2.07000, Rounded BF16: 2.06250\n",
      "Original FP32: 2.08000, Rounded BF16: 2.07812\n",
      "Original FP32: 2.09000, Rounded BF16: 2.09375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Generate numbers between 2 and 4 with small increments\n",
    "values_fp32 = torch.arange(2.0, 4.01, 0.01, dtype=torch.float32)\n",
    "values_bf16 = values_fp32.to(torch.bfloat16)\n",
    "\n",
    "# Print first few values to see the rounding effect\n",
    "for i in range(10):  # Display only first 10 for brevity\n",
    "    print(f\"Original FP32: {values_fp32[i].item():.5f}, Rounded BF16: {values_bf16[i].item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 195.25\n",
      "\n",
      "Float32 (IEEE 754) Representation: 0 00000000 10000000100001101000011\n",
      "BFloat16 Representation: 0 10000110 1000011\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import torch\n",
    "\n",
    "def float_to_fp32_bf16_repr(value):\n",
    "    # Convert to float32 (IEEE 754 Single Precision)\n",
    "    fp32_bytes = struct.pack('f', value)  # Convert float to IEEE 754 binary format\n",
    "    fp32_bits = ''.join(f'{b:08b}' for b in struct.unpack('4B', fp32_bytes))  # Convert bytes to bit string\n",
    "    \n",
    "    # Convert to bfloat16 (truncate lower 16 bits)\n",
    "    bf16_tensor = torch.tensor(value, dtype=torch.bfloat16)\n",
    "    bf16_bits = f'{bf16_tensor.view(torch.int16).item():016b}'  # Convert bfloat16 to bit string\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Value: {value}\\n\")\n",
    "    print(f\"Float32 (IEEE 754) Representation: {fp32_bits[:1]} {fp32_bits[1:9]} {fp32_bits[9:]}\")\n",
    "    print(f\"BFloat16 Representation: {bf16_bits[:1]} {bf16_bits[1:9]} {bf16_bits[9:]}\")\n",
    "\n",
    "# Test with 195.25\n",
    "float_to_fp32_bf16_repr(195.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 30.0\n",
      "  Float32 (IEEE 754): 0 10000011 11100000000000000000000\n",
      "  BFloat16:           0 10000011 1110000\n",
      "\n",
      "Value: 31.99999\n",
      "  Float32 (IEEE 754): 0 10000011 11111111111111111111011\n",
      "  BFloat16:           0 10000100 0000000\n",
      "\n",
      "Value: 32.0\n",
      "  Float32 (IEEE 754): 0 10000100 00000000000000000000000\n",
      "  BFloat16:           0 10000100 0000000\n",
      "\n",
      "Value: 32.000001\n",
      "  Float32 (IEEE 754): 0 10000100 00000000000000000000000\n",
      "  BFloat16:           0 10000100 0000000\n",
      "\n",
      "Value: 2\n",
      "  Float32 (IEEE 754): 0 10000000 00000000000000000000000\n",
      "  BFloat16:           0 10000000 0000000\n",
      "\n",
      "Value: 32.015625\n",
      "  Float32 (IEEE 754): 0 10000100 00000000001000000000000\n",
      "  BFloat16:           0 10000100 0000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import torch\n",
    "\n",
    "def float32_components(value: float):\n",
    "    \"\"\"\n",
    "    Returns the sign, exponent, and mantissa (23 bits) of a float32 number.\n",
    "    \"\"\"\n",
    "    # Pack the float into 32-bit binary (little-endian), then unpack as an unsigned int\n",
    "    bits = struct.unpack('<I', struct.pack('<f', value))[0]\n",
    "    sign = (bits >> 31) & 0x1\n",
    "    exponent = (bits >> 23) & 0xFF\n",
    "    mantissa = bits & 0x7FFFFF\n",
    "    return sign, exponent, mantissa\n",
    "\n",
    "def bf16_components(value: float):\n",
    "    \"\"\"\n",
    "    Returns the sign, exponent, and mantissa (7 bits) of a bfloat16 number.\n",
    "    \"\"\"\n",
    "    # Convert to bfloat16 using PyTorch, then interpret the 16 bits as an integer\n",
    "    bf16_val = torch.tensor(value, dtype=torch.bfloat16).view(torch.int16).item() & 0xFFFF\n",
    "    sign = (bf16_val >> 15) & 0x1\n",
    "    exponent = (bf16_val >> 7) & 0xFF\n",
    "    mantissa = bf16_val & 0x7F\n",
    "    return sign, exponent, mantissa\n",
    "\n",
    "def print_float32_bf16(value: float):\n",
    "    \"\"\"\n",
    "    Prints the binary representation of `value` in float32 (IEEE 754) and bfloat16.\n",
    "    \"\"\"\n",
    "    s32, e32, m32 = float32_components(value)\n",
    "    s_bf16, e_bf16, m_bf16 = bf16_components(value)\n",
    "    \n",
    "    # Build binary strings\n",
    "    float32_bin = f\"{s32} {e32:08b} {m32:023b}\"       # 1 + 8 + 23 = 32 bits\n",
    "    bf16_bin    = f\"{s_bf16} {e_bf16:08b} {m_bf16:07b}\" # 1 + 8 + 7 = 16 bits\n",
    "    \n",
    "    print(f\"Value: {value}\")\n",
    "    print(f\"  Float32 (IEEE 754): {float32_bin}\")\n",
    "    print(f\"  BFloat16:           {bf16_bin}\\n\")\n",
    "\n",
    "# Test values\n",
    "values = [30.0, 31.99999, 32.0, 32.000001,2 , 32.015625]\n",
    "for v in values:\n",
    "    print_float32_bf16(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
