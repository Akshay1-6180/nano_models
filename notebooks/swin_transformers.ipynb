{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/akshay.v/miniconda3/envs/nano_models/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from einops.layers.torch import Rearrange\n",
    "from einops import repeat, rearrange\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(img_src=\"/home/users/akshay.v/nano_models/images/guitar.jpg\", img_size = (224,224)):\n",
    "    image = cv2.imread(img_src)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, img_size, interpolation = cv2.INTER_CUBIC)\n",
    "    image = image.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "    image = torch.Tensor(image)\n",
    "    image = image.permute(2,0,1) # C , H ,W\n",
    "    image = image.unsqueeze(0) #to add the batch dimension\n",
    "    print(\"image shape is \" , image.shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPB_MLP(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_heads, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = [3, 3]  # Example window size\n",
    "relative_coords = torch.stack(torch.meshgrid(\n",
    "    torch.arange(-(window_size[0]-1), window_size[0]),\n",
    "    torch.arange(-(window_size[1]-1), window_size[1])\n",
    ")).permute(1, 2, 0).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2, -1,  0,  1,  2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(-(window_size[0]-1), window_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpb_mlp = CPB_MLP(num_heads=8)\n",
    "relative_position_bias = cpb_mlp(relative_coords.view(-1, 2).float()).view(\n",
    "    window_size[0]*2-1, window_size[1]*2-1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_position_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) https://bolster.ai/blog/swin-transformers\n",
    "2) https://anyline.com/news/transformers-in-computer-vision\n",
    "3) https://chautuankien.medium.com/explanation-swin-transformer-93e7a3140877\n",
    "4) https://www.youtube.com/watch?v=LxPDpAiyqSU&list=PL9iXGo3xD8jokWaLB8ZHUkjjv5Y_vPQnZ&index=2&ab_channel=AIOpenCourseWare\n",
    "5) https://medium.com/thedeephub/building-swin-transformer-from-scratch-using-pytorch-hierarchical-vision-transformer-using-shifted-91cbf6abc678\n",
    "6) https://towardsdatascience.com/a-comprehensive-guide-to-swin-transformer-64965f89d14c\n",
    "7) https://www.youtube.com/watch?v=SndHALawoag&ab_channel=AICoffeeBreakwithLetitia\n",
    "\n",
    "The code is inspired from\n",
    "1) https://github.com/berniwal/swin-transformer-pytorch\n",
    "2) https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with VIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we take each each patch as 1*1 and then calculate the number of tokens , we get around 63000 tokens and for larger images of size 1024 it goes to over a million tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(nn.Module):\n",
    "    def __init__(self, patch_size=56, stride_size=56):\n",
    "        super().__init__()\n",
    "        self.p = patch_size\n",
    "        self.unfold = torch.nn.Unfold(kernel_size=patch_size, stride=stride_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x -> B c h w\n",
    "        bs, c, h, w = x.shape\n",
    "        \n",
    "        x = self.unfold(x)\n",
    "        # x -> B (c*p*p) L\n",
    "        print(x.shape)\n",
    "        # Reshaping into the shape we want\n",
    "        a = x.view(bs, c, self.p, self.p, -1).permute(0, 4, 1, 2, 3)\n",
    "        # a -> ( B no.of patches c p p )\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 3, 65536])\n",
      "torch.Size([1, 65536, 3, 1, 1])\n",
      "torch.Size([1, 65536, 3])\n"
     ]
    }
   ],
   "source": [
    "patch = Patchify(patch_size=1, stride_size=1)\n",
    "img_src = \"/home/users/akshay.v/nano_models/images/guitar.jpg\"\n",
    "image = cv2.imread(img_src)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (256, 256), interpolation = cv2.INTER_CUBIC)\n",
    "image = image.astype('float32') / 255.0  # Normalize to [0, 1]\n",
    "image = torch.Tensor(image)\n",
    "image = image.permute(2,0,1) # C , H ,W\n",
    "image = image.unsqueeze(0) #to add the batch dimension\n",
    "print(image.shape)\n",
    "p = patch(image)\n",
    "print(p.shape)\n",
    "p = p.squeeze() #to remove the batch dimension for plotting\n",
    "print(p.reshape(1,p.shape[0],-1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformers\n",
    "Will be going through the swin easy code \n",
    "The swin offical code is very simiar to this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see different ways of how patch merging is implemented , u might notice for ex (1, 96, 192, 284) in the code , these show in the changes in size as it passed through the layers\n",
    "\n",
    "There are 3 ways its implemented\n",
    "1) using unfold + linear layer\n",
    "2) using a conv layer\n",
    "3) using a patch embed and patch merge , in the case of the offical swin repo\n",
    "\n",
    "I prefer the conv layer one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using unfold + linear\n",
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0) \n",
    "        # will just conver it from 1, 3, 224 ,224 into 1, (4*4*3), (56*56)\n",
    "        # this will get resized to 1, 48, 56, 56 \n",
    "        # if we take the above one and reshape it to 1, 56, 56 ,4 , 4, 3 we can print the entire image in patch of 4\n",
    "        # this will get resized to 1, 56, 56, 48\n",
    "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor\n",
    "        \n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
    "        print(x.shape)\n",
    "        x = self.linear(x) # [1, 56, 56, 96]\n",
    "        return x\n",
    "    \n",
    "# using conv\n",
    "class PatchMerging_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
    "        super().__init__()\n",
    "        self.downscaling_factor = downscaling_factor\n",
    "        self.patch_merge = nn.Conv2d(in_channels=in_channels, \n",
    "                                     out_channels=out_channels, \n",
    "                                     kernel_size=downscaling_factor, \n",
    "                                     stride=downscaling_factor, padding=0)\n",
    "        \n",
    "        \"\"\"\n",
    "        Each filter in this layer is a 4x4 filter that extends through the depth of the input volume. \n",
    "        Since the input volume has 3 channels, each filter actually has a dimension of 4x4x3 (width x height x depth). \n",
    "        There are 96 such filters, each producing one feature map, leading to an output volume with 96 channels.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape # [1, (3, 96, 192, 384), (224, 56, 28, 14), (224, 56, 28, 14)]\n",
    "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor # 224/4 = 56 # (56,28,17,7)\n",
    "        print(self.patch_merge(x).shape)\n",
    "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1) # # ( 1, (56, 28, 14, 7), (56, 28, 14, 7), (96, 192, 384, 768)]\n",
    "        print(type(self).__name__, \"size of image after conv\" , x.shape)\n",
    "        return x\n",
    "\n",
    "# official swin repo replementation   \n",
    "\n",
    "class PatchEmbed_Official(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "    \n",
    "class PatchMerging_Official(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        print(x.shape)\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "\n",
    "        ## basically here it \n",
    "\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        # conatentate along the long dimension\n",
    "        print(x.shape)\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        print(x.shape)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape is  torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "image = read_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56, 56, 48])\n"
     ]
    }
   ],
   "source": [
    "patch_merge = PatchMerging(3,96,4)\n",
    "out = patch_merge(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 56, 56])\n",
      "PatchMerging_Conv size of image after conv torch.Size([1, 56, 56, 96])\n"
     ]
    }
   ],
   "source": [
    "patch_merge = PatchMerging_Conv(3,96,4)\n",
    "out = patch_merge(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/patch_merging.png\" alt=\"alt text\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136, 96])\n",
      "torch.Size([1, 56, 56, 96])\n",
      "torch.Size([1, 28, 28, 384])\n",
      "torch.Size([1, 784, 384])\n",
      "torch.Size([1, 784, 192])\n",
      "torch.Size([1, 784, 192])\n"
     ]
    }
   ],
   "source": [
    "# in the case of the official repo they do a patch embed which converts 1 , 3, 224, 224 into the image size of 1, 56, 56, 96\n",
    "# which is then passed into a patch merging function unlike the other code  after the first swin block to downsample it to 1, 28, 28, 196\n",
    "module = PatchEmbed_Official()\n",
    "out_image = module(image)\n",
    "print(out_image.shape)\n",
    "patch_merge = PatchMerging_Official((56,56),96)\n",
    "out = patch_merge(out_image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28, 384])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to understand the stack operation \n",
    "test = torch.randn(1,28,28,96)\n",
    "# basically stacks all of them along to the last dimension \n",
    "out_test = torch.cat([test,test,test,test], dim=-1)\n",
    "out_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will only be focusing on the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.swin_easy import SwinBlock, PatchMerging\n",
    "class StageModule(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
    "                 relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
    "\n",
    "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
    "                                            downscaling_factor=downscaling_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(layers // 2):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.patch_partition(x)\n",
    "        print(x.shape)\n",
    "        for regular_block, shifted_block in self.layers:\n",
    "            x = regular_block(x)\n",
    "            x = shifted_block(x)\n",
    "        #print(x.shape)\n",
    "        return x.permute(0, 3, 1, 2)\n",
    "    \n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, *, hidden_dim, layers, heads, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
    "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
    "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
    "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
    "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 8),\n",
    "            nn.Linear(hidden_dim * 8, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img = (1, 3, 224, 224)\n",
    "        x = self.stage1(img)\n",
    "        #After permute from stage block x = (1, 96, 56, 56)\n",
    "        x = self.stage2(x)\n",
    "        #After permute from stage block x = (1, 192, 28, 28)\n",
    "        x = self.stage3(x)\n",
    "        print(x.shape)\n",
    "        #After permute from stage block x = (1, 384, 14, 14)\n",
    "        x = self.stage4(x)\n",
    "        print(x.shape)\n",
    "        #After permute from stage block x = (1, 768, 7, 7)\n",
    "        x = x.mean(dim=[2, 3])\n",
    "        return self.mlp_head(x)\n",
    "\n",
    "\n",
    "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
    "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 56, 56, 96])\n",
      "torch.Size([1, 96, 56, 56])\n",
      "torch.Size([1, 28, 28, 192])\n",
      "torch.Size([1, 192, 28, 28])\n",
      "torch.Size([1, 14, 14, 384])\n",
      "torch.Size([1, 384, 14, 14])\n",
      "torch.Size([1, 384, 14, 14])\n",
      "torch.Size([1, 7, 7, 768])\n",
      "torch.Size([1, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "model = swin_t()\n",
    "out = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage1 = StageModule(3,96,layers=2,downscaling_factor=4,num_heads=3,window_size=7,relative_pos_embedding=True)\n",
    "stage2 = StageModule(96,96*2,layers=2,downscaling_factor=2,num_heads=6,window_size=7,relative_pos_embedding=True)\n",
    "stage3 = StageModule(96*2,96*4,layers=6,downscaling_factor=2,num_heads=12,window_size=7,relative_pos_embedding=True)\n",
    "stage4 = StageModule(96*4,96*8,layers=2,downscaling_factor=2,num_heads=24,window_size=7,relative_pos_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape is  torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 56, 56, 96])\n",
      "torch.Size([1, 56, 56, 96])\n"
     ]
    }
   ],
   "source": [
    "in_channels=3\n",
    "hidden_dim = 96\n",
    "layers=(2, 2, 6, 2)\n",
    "heads=(3, 6, 12, 24)\n",
    "head_dim=32\n",
    "window_size=7\n",
    "downscaling_factors=(4, 2, 2, 2)\n",
    "relative_pos_embedding=True\n",
    "stage1 = StageModule(in_channels=in_channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
    "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
    "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "image = read_image()\n",
    "out_1 = stage1(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "\n",
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2) #49,49\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        \"\"\"\n",
    "        dim = hidden_dim = (96,192,384,768)\n",
    "        heads = num_heads = (3,6,12,24)\n",
    "        head_dim=32\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads # (32*3 = 96, 32*6=192, 32*12=384, 32*24=768)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5 # scalling dot product inside the softmax\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)  \n",
    "        # dim = (96,192,384,768)\n",
    "        # inner_dim = head_dim * 3\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1)) # 13,13\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            # x shape is (1, (56,28,14,7), (56,28,14,7), (96,192,384, 768)\n",
    "            x = self.cyclic_shift(x)\n",
    "            # x shape is (1, (56,28,14,7), (56,28,14,7), (96,192,384, 768)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) # basically divided into 3 for q, k , v\n",
    "        nw_h = n_h // self.window_size  # 56/7 = 8 , (8,4, 2, 1 )\n",
    "        nw_w = n_w // self.window_size # (8,4, 2, 1 )\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "        # shape of q, k ,v is \n",
    "        # (1, h=(3,6,12,24), (nw_h*nw_w)= (64,16,4,1), (w_h*w_w)=49, d = 32)\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "        # (1, h=(3,6,12,24), (w)= (64,16,4,1), (i,j)=49)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of swinv1 , layer norm is applied before attention and feedforward\n",
    "\n",
    "In the case of swinv2 . layer norm is applied after attention and  feedforward\n",
    "\n",
    "<img src=\"../images/swin_v1_v2.png\" alt=\"alt text\" width=\"500\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "    \n",
    "class PostNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.norm(self.fn(x, **kwargs))\n",
    "    \n",
    "#swin v1\n",
    "class SwinBlock_v1(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x\n",
    "    \n",
    "#swin v2\n",
    "class SwinBlock_v2(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PostNorm(dim, WindowAttention(dim=dim,\n",
    "                                                                     heads=heads,\n",
    "                                                                     head_dim=head_dim,\n",
    "                                                                     shifted=shifted,\n",
    "                                                                     window_size=window_size,\n",
    "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PostNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x\n",
    "\n",
    "# we include cosine attention in swinv2\n",
    "import torch.nn.functional as F\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        \"\"\"\n",
    "        dim = hidden_dim = (96,192,384,768)\n",
    "        heads = num_heads = (3,6,12,24)\n",
    "        head_dim=32\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads # (32*3 = 96, 32*6=192, 32*12=384, 32*24=768)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5 # scalling dot product inside the softmax\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "        self.shifted = shifted\n",
    "\n",
    "        self.tau = nn.Parameter(torch.tensor(0.01))\n",
    "        # for simplicity but in the orginal paper each head and each layer have a different tau\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
    "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)  \n",
    "        # dim = (96,192,384,768)\n",
    "        # inner_dim = head_dim * 3\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1)) # 13,13\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            # x shape is (1, (56,28,14,7), (56,28,14,7), (96,192,384, 768)\n",
    "            x = self.cyclic_shift(x)\n",
    "            # x shape is (1, (56,28,14,7), (56,28,14,7), (96,192,384, 768)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) # basically divided into 3 for q, k , v\n",
    "        nw_h = n_h // self.window_size  # 56/7 = 8 , (8,4, 2, 1 )\n",
    "        nw_w = n_w // self.window_size # (8,4, 2, 1 )\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
    "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "        # shape of q, k ,v is \n",
    "        # (1, h=(3,6,12,24), (nw_h*nw_w)= (64,16,4,1), (w_h*w_w)=49, d = 32)\n",
    "        # First normalizing q and k with respect to each row\n",
    "        q = F.normalize(q, p=2, dim=-1)\n",
    "        k = F.normalize(k, p=2, dim=-1)\n",
    "\n",
    "        # Cosine Similarity\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) / self.tau\n",
    "        # b=batch_size, h=heads (3, 6, 12, 24), w=width (64, 16, 4, 1), i=j=49\n",
    "\n",
    "        #dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
    "        # (1, h=(3,6,12,24), (w)= (64,16,4,1), (i,j)=49)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
    "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.2792, -0.7208, -1.3164,  1.9773,  2.3340])\n",
      "tensor([ 0.5560, -0.1758, -0.3211,  0.4823,  0.5694])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# how cosine similariy works for q, k \n",
    "import torch\n",
    "q = torch.randn(1, 64, 49, 5)\n",
    "print(q[0][0][0])\n",
    "q = F.normalize(q, p=2, dim=-1) # v/|v|\n",
    "print(q[0][0][0])\n",
    "#magnitude should be 1 since its normalise\n",
    "print(torch.norm(q[0][0][0], p=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take the first block and see it \n",
    "so \n",
    "stage1 = StageModule(3,96,layers=2,downscaling_factor=4,num_heads=3,window_size=7,relative_pos_embedding=True)\n",
    "\n",
    "so there would be 2 layers , one of them the normal MSA and other other one would be SH-MSA\n",
    "\n",
    "the output would be after the patch merging and then sending it to swin block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape is  torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 56, 56, 96])\n"
     ]
    }
   ],
   "source": [
    "from modules.swin_easy import SwinBlock, PatchMerging\n",
    "image = read_image()\n",
    "patch_module =  PatchMerging(3,96,4)\n",
    "out = patch_module(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dimension = 96\n",
    "num_heads = 3\n",
    "head_dim = 32\n",
    "window_size = 7\n",
    "relative_pos_embedding = True\n",
    "\n",
    "msa_module = SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
    "\n",
    "shmsa_module = SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
    "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56, 56, 96])\n",
      "torch.Size([1, 56, 56, 96])\n"
     ]
    }
   ],
   "source": [
    "msa_out = msa_module(out)\n",
    "print(msa_out.shape)\n",
    "shmsa_out = shmsa_module(msa_out)\n",
    "print(shmsa_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand torch.roll lets consider a (1,81,81) and shift it and see how it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.],\n",
      "        [10., 11., 12., 13., 14., 15., 16., 17., 18.],\n",
      "        [19., 20., 21., 22., 23., 24., 25., 26., 27.],\n",
      "        [28., 29., 30., 31., 32., 33., 34., 35., 36.],\n",
      "        [37., 38., 39., 40., 41., 42., 43., 44., 45.],\n",
      "        [46., 47., 48., 49., 50., 51., 52., 53., 54.],\n",
      "        [55., 56., 57., 58., 59., 60., 61., 62., 63.],\n",
      "        [64., 65., 66., 67., 68., 69., 70., 71., 72.],\n",
      "        [73., 74., 75., 76., 77., 78., 79., 80., 81.]])\n",
      "\n",
      "Shifted tensor:\n",
      "tensor([[11., 12., 13., 14., 15., 16., 17., 18., 10.],\n",
      "        [20., 21., 22., 23., 24., 25., 26., 27., 19.],\n",
      "        [29., 30., 31., 32., 33., 34., 35., 36., 28.],\n",
      "        [38., 39., 40., 41., 42., 43., 44., 45., 37.],\n",
      "        [47., 48., 49., 50., 51., 52., 53., 54., 46.],\n",
      "        [56., 57., 58., 59., 60., 61., 62., 63., 55.],\n",
      "        [65., 66., 67., 68., 69., 70., 71., 72., 64.],\n",
      "        [74., 75., 76., 77., 78., 79., 80., 81., 73.],\n",
      "        [ 2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Create a sample tensor\n",
    "x = torch.linspace(1,81,81).view(9,9)\n",
    "# Roll the input tensor along the specified dimensions\n",
    "shifted_x = torch.roll(x, shifts=(-1, -1), dims=(0, 1))\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "print(\"\\nShifted tensor:\")\n",
    "print(shifted_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(window_size, displacement, upper_lower, left_right):\n",
    "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
    "\n",
    "    if upper_lower:\n",
    "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
    "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
    "\n",
    "    if left_right:\n",
    "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
    "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
    "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
    "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0.],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3 \n",
    "displacement = 1\n",
    "upper_lower=True\n",
    "left_right=False\n",
    "create_mask(window_size, displacement, upper_lower, left_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 3 \n",
    "displacement = 1\n",
    "upper_lower=False\n",
    "left_right=True\n",
    "create_mask(window_size, displacement, upper_lower, left_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., -inf, 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [-inf, -inf, 0., -inf, -inf, 0., -inf, -inf, -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, 0., 0., -inf],\n",
       "        [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_lower=True\n",
    "left_right=True\n",
    "create_mask(window_size, displacement, upper_lower, left_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  0, -1, -1],\n",
      "         [ 0,  0, -1, -1],\n",
      "         [ 1,  1,  0,  0],\n",
      "         [ 1,  1,  0,  0]],\n",
      "\n",
      "        [[ 0, -1,  0, -1],\n",
      "         [ 1,  0,  1,  0],\n",
      "         [ 0, -1,  0, -1],\n",
      "         [ 1,  0,  1,  0]]])\n",
      "tensor([[ 0, -1, -1, -2],\n",
      "        [ 1,  0,  0, -1],\n",
      "        [ 1,  0,  0, -1],\n",
      "        [ 2,  1,  1,  0]])\n",
      "tensor([[ 1,  0,  0, -1],\n",
      "        [ 2,  1,  1,  0],\n",
      "        [ 2,  1,  1,  0],\n",
      "        [ 3,  2,  2,  1]])\n",
      "tensor([[2, 1, 1, 0],\n",
      "        [3, 2, 2, 1],\n",
      "        [3, 2, 2, 1],\n",
      "        [4, 3, 3, 2]])\n",
      "tensor([[4, 3, 1, 0],\n",
      "        [5, 4, 2, 1],\n",
      "        [7, 6, 4, 3],\n",
      "        [8, 7, 5, 4]])\n"
     ]
    }
   ],
   "source": [
    "#how the relative positiional encoding works\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "window_size = (2,2)\n",
    "num_heads = 3\n",
    "relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "# get pair-wise relative position index for each token inside the window\n",
    "coords_h = torch.arange(window_size[0])\n",
    "coords_w = torch.arange(window_size[1])\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "print(relative_coords)\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "print(relative_coords.sum(-1))\n",
    "\"\"\"\n",
    "This step adjusts the relative height coordinates to ensure they are non-negative. By adding window_size[0] - 1 to the height component (index 0 in the last dimension), \n",
    "the minimum possible value is shifted to 0. This adjustment is necessary because the relative positions can be negative (e.g., a token above another token), \n",
    "and we want to map these to positive indices\n",
    "\"\"\"\n",
    "relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "\n",
    "print(relative_coords.sum(-1))\n",
    "\"\"\"\n",
    "This step adjusts the relative height coordinates to ensure they are non-negative. By adding window_size[0] - 1 to the height component (index 0 in the last dimension), \n",
    "the minimum possible value is shifted to 0. This adjustment is necessary because the relative positions can be negative (e.g., a token above another token), \n",
    "and we want to map these to positive indices\n",
    "\"\"\"\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "print(relative_coords.sum(-1))\n",
    "\"\"\"\n",
    "The adjustment of the height component is scaled by 2 * window_size[1] - 1 to ensure that when we sum the height and width adjustments, each unique (height, width) pair maps to a unique index.\n",
    "This scaling is crucial because it spreads out the indices for the height differences across a larger range than the width differences, ensuring no two different (height, width) \n",
    "pairs end up with the same sum.\n",
    "\"\"\"\n",
    "relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "print(relative_position_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 1, 0, 5, 4, 2, 1, 7, 6, 4, 3, 8, 7, 5, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_position_index.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would be a trainable parameter and relative_position_index helps us in getting the relevant bias terms for each window\n",
    "relative_position_bias = relative_position_bias_table[relative_position_index.view(-1)].view(4, 4, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.],\n",
       "         [0., 0., 0.]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_position_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 9, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relative_distances(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1, -1],\n",
      "          [-1,  0],\n",
      "          [-1,  1]],\n",
      "\n",
      "         [[ 0, -1],\n",
      "          [ 0,  0],\n",
      "          [ 0,  1]],\n",
      "\n",
      "         [[ 1, -1],\n",
      "          [ 1,  0],\n",
      "          [ 1,  1]]]])\n",
      "torch.Size([1, 3, 3, 2])\n",
      "tensor([[[ 0,  0, -1, -1],\n",
      "         [ 0,  0, -1, -1],\n",
      "         [ 1,  1,  0,  0],\n",
      "         [ 1,  1,  0,  0]],\n",
      "\n",
      "        [[ 0, -1,  0, -1],\n",
      "         [ 1,  0,  1,  0],\n",
      "         [ 0, -1,  0, -1],\n",
      "         [ 1,  0,  1,  0]]])\n",
      "tensor([[ 0, -1, -1, -2],\n",
      "        [ 1,  0,  0, -1],\n",
      "        [ 1,  0,  0, -1],\n",
      "        [ 2,  1,  1,  0]])\n",
      "tensor([[ 1,  0,  0, -1],\n",
      "        [ 2,  1,  1,  0],\n",
      "        [ 2,  1,  1,  0],\n",
      "        [ 3,  2,  2,  1]])\n",
      "tensor([[2, 1, 1, 0],\n",
      "        [3, 2, 2, 1],\n",
      "        [3, 2, 2, 1],\n",
      "        [4, 3, 3, 2]])\n",
      "tensor([[4, 3, 1, 0],\n",
      "        [5, 4, 2, 1],\n",
      "        [7, 6, 4, 3],\n",
      "        [8, 7, 5, 4]])\n"
     ]
    }
   ],
   "source": [
    "#for swinv2\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CPB_MLP(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(2, 512, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_heads, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "window_size = (2,2)\n",
    "num_heads = 3\n",
    "relative_coords_table = torch.stack(torch.meshgrid(\n",
    "    torch.arange(-(window_size[0]-1), window_size[0]),\n",
    "    torch.arange(-(window_size[1]-1), window_size[1])\n",
    ")).permute(1, 2, 0).unsqueeze(0)\n",
    "print(relative_coords_table)\n",
    "relative_coords_table[:, :, :, 0] = relative_coords_table[:, :, :, 0]/(window_size[0] - 1)\n",
    "relative_coords_table[:, :, :, 1] = relative_coords_table[:, :, :, 1]/(window_size[0] - 1)\n",
    "relative_coords_table *= 8  # normalize to -8, 8\n",
    "relative_coords_table = torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n",
    "print(relative_coords_table.shape)\n",
    "# get pair-wise relative position index for each token inside the window\n",
    "coords_h = torch.arange(window_size[0])\n",
    "coords_w = torch.arange(window_size[1])\n",
    "coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "print(relative_coords)\n",
    "relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "print(relative_coords.sum(-1))\n",
    "\"\"\"\n",
    "This step adjusts the relative height coordinates to ensure they are non-negative. By adding window_size[0] - 1 to the height component (index 0 in the last dimension), \n",
    "the minimum possible value is shifted to 0. This adjustment is necessary because the relative positions can be negative (e.g., a token above another token), \n",
    "and we want to map these to positive indices\n",
    "\"\"\"\n",
    "relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n",
    "\n",
    "print(relative_coords.sum(-1))\n",
    "\"\"\"\n",
    "This step adjusts the relative height coordinates to ensure they are non-negative. By adding window_size[0] - 1 to the height component (index 0 in the last dimension), \n",
    "the minimum possible value is shifted to 0. This adjustment is necessary because the relative positions can be negative (e.g., a token above another token), \n",
    "and we want to map these to positive indices\n",
    "\"\"\"\n",
    "relative_coords[:, :, 1] += window_size[1] - 1\n",
    "print(relative_coords.sum(-1))\n",
    "\"\"\"\n",
    "The adjustment of the height component is scaled by 2 * window_size[1] - 1 to ensure that when we sum the height and width adjustments, each unique (height, width) pair maps to a unique index.\n",
    "This scaling is crucial because it spreads out the indices for the height differences across a larger range than the width differences, ensuring no two different (height, width) \n",
    "pairs end up with the same sum.\n",
    "\"\"\"\n",
    "relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n",
    "relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "print(relative_position_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.0566, -1.0566],\n",
       "           [-1.0566,  0.0000],\n",
       "           [-1.0566,  1.0566]],\n",
       " \n",
       "          [[ 0.0000, -1.0566],\n",
       "           [ 0.0000,  0.0000],\n",
       "           [ 0.0000,  1.0566]],\n",
       " \n",
       "          [[ 1.0566, -1.0566],\n",
       "           [ 1.0566,  0.0000],\n",
       "           [ 1.0566,  1.0566]]]]),\n",
       " torch.Size([1, 3, 3, 2]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_coords_table , relative_coords_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 2])\n",
      "tensor([[ 0.0844, -0.2220,  0.0212],\n",
      "        [ 0.1006,  0.0455, -0.0113],\n",
      "        [ 0.3036,  0.1925, -0.0254],\n",
      "        [ 0.0752, -0.3241,  0.0239],\n",
      "        [ 0.0620, -0.1128,  0.0072],\n",
      "        [ 0.2428,  0.0898, -0.0980],\n",
      "        [ 0.2094, -0.2668,  0.1333],\n",
      "        [ 0.0194, -0.2149,  0.0088],\n",
      "        [ 0.0231, -0.0200, -0.0982]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([9, 3])\n",
      "torch.Size([4, 4, 3])\n",
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "num_heads = 3\n",
    "cpb_mlp = CPB_MLP(num_heads)\n",
    "print(relative_coords_table.shape)\n",
    "relative_position_bias_table = cpb_mlp(relative_coords_table).view(-1, num_heads) # 9, 3]\n",
    "print(relative_position_bias_table)\n",
    "print(relative_position_bias_table.shape)\n",
    "relative_position_bias = relative_position_bias_table[relative_position_index.view(-1)].view(window_size[0] * window_size[1], window_size[0] * window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "print(relative_position_bias.shape)\n",
    "relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "print(relative_position_bias.shape)\n",
    "relative_position_bias = 16 * torch.sigmoid(relative_position_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 1, 0, 5, 4, 2, 1, 7, 6, 4, 3, 8, 7, 5, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_position_index.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets understadn this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Logarithmic Transformation and Its Non-linearity</b>\n",
    "\n",
    "The logarithmic function is inherently non-linear, characterized by its unique rate of growth: it increases rapidly for small positive values but decelerates as the values become larger. This property of the logarithmic function makes it especially useful for processing relative positions in data, allowing for a more nuanced handling of positional relationships. Let's delve into how this non-linearity manifests and its implications for modeling positional relationships.\n",
    "\n",
    "<b> Understanding Logarithmic Growth</b>\n",
    "\n",
    "Logarithmic growth is defined by the relationship between a number $x$ (for $x > 0$) and the power to which a base (commonly 2, e, or 10) must be raised to yield $x$. For example, using base 2, $\\log_2(2) = 1$ because $2^1 = 2$, and $\\log_2(8) = 3$ because $2^3 = 8$. The logarithm of a number thus reflects its order of magnitude relative to the base.\n",
    "\n",
    "- **Rapid Growth for Small Values**: The logarithmic function grows more quickly for values of $x$ close to zero. In the context of relative positions, this means that small differences (e.g., between positions 1 and 2) are accentuated after the logarithmic transformation, providing \"more granularity\" to these distances. This fine distinction allows models to discern closely positioned elements with greater precision.\n",
    "\n",
    "- **Slower Growth for Larger Values**: As $x$ becomes larger, the rate at which its logarithm increases slows down. This aspect of logarithmic scaling means that larger distances (e.g., between positions 100 and 101) are compressed into almost identical values after transformation. Such \"compression\" groups distant positions together, allowing the model to treat them as nearly equivalent and to prioritize the analysis of closer positional interactions.\n",
    "\n",
    "<b>  Implications for Positional Relationships in NLP and CV</b>\n",
    "\n",
    "In many domains, such as NLP and CV, the interactions between elements that are close to each other often have more significance than interactions between elements that are far apart. For instance, in textual data, the context and meaning of a word are more directly influenced by its immediate neighbors than by words in a distant sentence. Similarly, in images, pixels are more strongly related to adjacent pixels than to those far across the image.\n",
    "\n",
    "Applying a logarithmic transformation to relative positions allows models to mimic this distribution of significance, emphasizing the importance of nearer interactions over distant ones. By doing so, computational resources can be focused more efficiently on the most meaningful relationships in the data, potentially enhancing the model's ability to learn and make predictions about the structure and content of the input.\n",
    "\n",
    "In summary, the logarithmic function, through its non-linear scaling, provides a mechanism for models to discriminate finely among close relationships while consolidating distant ones, aligning model processing more closely with the inherent structure of real-world data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1, -1],\n",
      "          [-1,  0],\n",
      "          [-1,  1]],\n",
      "\n",
      "         [[ 0, -1],\n",
      "          [ 0,  0],\n",
      "          [ 0,  1]],\n",
      "\n",
      "         [[ 1, -1],\n",
      "          [ 1,  0],\n",
      "          [ 1,  1]]]])\n",
      "tensor([[[[-1, -1],\n",
      "          [-1,  0],\n",
      "          [-1,  1]],\n",
      "\n",
      "         [[ 0, -1],\n",
      "          [ 0,  0],\n",
      "          [ 0,  1]],\n",
      "\n",
      "         [[ 1, -1],\n",
      "          [ 1,  0],\n",
      "          [ 1,  1]]]])\n",
      "tensor([[[[-8, -8],\n",
      "          [-8,  0],\n",
      "          [-8,  8]],\n",
      "\n",
      "         [[ 0, -8],\n",
      "          [ 0,  0],\n",
      "          [ 0,  8]],\n",
      "\n",
      "         [[ 8, -8],\n",
      "          [ 8,  0],\n",
      "          [ 8,  8]]]])\n",
      "tensor([[[[-1.0566, -1.0566],\n",
      "          [-1.0566,  0.0000],\n",
      "          [-1.0566,  1.0566]],\n",
      "\n",
      "         [[ 0.0000, -1.0566],\n",
      "          [ 0.0000,  0.0000],\n",
      "          [ 0.0000,  1.0566]],\n",
      "\n",
      "         [[ 1.0566, -1.0566],\n",
      "          [ 1.0566,  0.0000],\n",
      "          [ 1.0566,  1.0566]]]])\n"
     ]
    }
   ],
   "source": [
    "window_size = (2,2)\n",
    "num_heads = 3\n",
    "relative_coords_table = torch.stack(torch.meshgrid(\n",
    "    torch.arange(-(window_size[0]-1), window_size[0]),\n",
    "    torch.arange(-(window_size[1]-1), window_size[1])\n",
    ")).permute(1, 2, 0).unsqueeze(0)\n",
    "print(relative_coords_table)\n",
    "relative_coords_table[:, :, :, 0] = relative_coords_table[:, :, :, 0]/(window_size[0] - 1)\n",
    "relative_coords_table[:, :, :, 1] = relative_coords_table[:, :, :, 1]/(window_size[0] - 1)\n",
    "print(relative_coords_table)\n",
    "relative_coords_table *= 8  # normalize to -8, 8\n",
    "print(relative_coords_table)\n",
    "relative_coords_table = torch.sign(relative_coords_table) * torch.log2(torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n",
    "print(relative_coords_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get based on swin easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  1,  1],\n",
      "        [ 0,  0,  1,  1],\n",
      "        [-1, -1,  0,  0],\n",
      "        [-1, -1,  0,  0]])\n",
      "tensor([[ 0,  1,  0,  1],\n",
      "        [-1,  0, -1,  0],\n",
      "        [ 0,  1,  0,  1],\n",
      "        [-1,  0, -1,  0]])\n",
      "tensor([[1, 1, 2, 2],\n",
      "        [1, 1, 2, 2],\n",
      "        [0, 0, 1, 1],\n",
      "        [0, 0, 1, 1]])\n",
      "tensor([[1, 2, 1, 2],\n",
      "        [0, 1, 0, 1],\n",
      "        [1, 2, 1, 2],\n",
      "        [0, 1, 0, 1]])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1278,  0.8391,  0.1339],\n",
      "        [ 0.1860, -0.4751,  1.4897],\n",
      "        [-0.0032,  1.7040,  0.8556]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4751,  1.4897,  1.7040,  0.8556],\n",
       "        [ 0.1860, -0.4751, -0.0032,  1.7040],\n",
       "        [ 0.8391,  0.1339, -0.4751,  1.4897],\n",
       "        [ 0.1278,  0.8391,  0.1860, -0.4751]], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances\n",
    "\n",
    "window_size = 2\n",
    "tmp = get_relative_distances(2)\n",
    "print(tmp[:,:,0])\n",
    "print(tmp[:,:,1])\n",
    "tmp = get_relative_distances(2) + window_size - 1\n",
    "print(tmp[:,:,0])\n",
    "print(tmp[:,:,1])\n",
    "\n",
    "pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "print(pos_embedding)\n",
    "pos_embedding[tmp[:,:,0], tmp[:,:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([1, 2, 3])\n",
      "size of a:  torch.Size([3])\n",
      "a1:  tensor([[1, 2, 3]])\n",
      "size of a1:  torch.Size([1, 3])\n",
      "a2:  tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "size of a2:  torch.Size([3, 1])\n",
      "d:  tensor([[-2, -3, -4],\n",
      "        [-3, -4, -5],\n",
      "        [-4, -5, -6]])\n",
      "size of d:  torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "print('a: ', a)\n",
    "print('size of a: ', a.size())\n",
    "a1 = a[None, :]\n",
    "print('a1: ', a1)\n",
    "print('size of a1: ', a1.size())\n",
    "a2 = a[:, None]\n",
    "print('a2: ', a2)\n",
    "print('size of a2: ', a2.size())\n",
    "d = -a1 - a2\n",
    "print('d: ', d)\n",
    "print('size of d: ', d.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to change its window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoConfig\n",
    "config = AutoConfig.from_pretrained(f'microsoft/swinv2-base-patch4-window8-256')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.window_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained(f'microsoft/swinv2-base-patch4-window8-256',config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For window size 2x2 u need 9 positonal bias <br>\n",
    "For window size 3x3 u need 25 positionl bias  <br>\n",
    "then we need to do bicubic interpolation but in swinv2 it handles it itself since the mlp layer takes care of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example input\n",
    "relative_coords_table = torch.randn(3, 3, 2)\n",
    "\n",
    "# Adjust the data to fit the expected input shape of interpolate(), i.e., (batch_size, channels, height, width)\n",
    "# Here, we treat the last dimension as channels and add a fake batch dimension\n",
    "relative_coords_table = relative_coords_table.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Apply bicubic interpolation\n",
    "# target_size is (5, 5) for the spatial dimensions (height and width)\n",
    "target_size = (5, 5)\n",
    "interpolated = F.interpolate(relative_coords_table, size=target_size, mode='bicubic', align_corners=True)\n",
    "\n",
    "# Remove the fake batch dimension and put the channels back to the last dimension\n",
    "interpolated = interpolated.squeeze(0).permute(1, 2, 0)\n",
    "\n",
    "print(interpolated.shape)  # Should be torch.Size([5, 5, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8989, -0.9325, -0.4685],\n",
       "          [ 0.9616,  0.9086, -0.9159],\n",
       "          [ 1.3957,  1.4294,  0.1827]],\n",
       "\n",
       "         [[ 0.1387, -0.7123, -0.2820],\n",
       "          [-0.2480,  0.7367,  0.4269],\n",
       "          [-0.5558, -0.0121,  1.1142]]]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relative_coords_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8989, -0.0603, -0.9325, -0.8722, -0.4685],\n",
       "          [ 0.8896,  0.4832, -0.0608, -0.5171, -0.7952],\n",
       "          [ 0.9616,  1.1061,  0.9086, -0.0086, -0.9159],\n",
       "          [ 1.1845,  1.4271,  1.3416,  0.4813, -0.4085],\n",
       "          [ 1.3957,  1.5294,  1.4294,  0.8092,  0.1827]],\n",
       "\n",
       "         [[ 0.1387, -0.3271, -0.7123, -0.5769, -0.2820],\n",
       "          [-0.0258,  0.0353,  0.0824,  0.0553,  0.0080],\n",
       "          [-0.2480,  0.2734,  0.7367,  0.6741,  0.4269],\n",
       "          [-0.4382, -0.0018,  0.4981,  0.7553,  0.8370],\n",
       "          [-0.5558, -0.3895, -0.0121,  0.6021,  1.1142]]]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated.permute(2, 0, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
